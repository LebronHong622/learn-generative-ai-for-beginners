{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474fc5a8",
   "metadata": {},
   "source": [
    "# 提示词工程介绍\n",
    "提示词工程是指**设计**和**优化**对大模型的输入内容，其目的让模型能得到一个最优的输出，这是一个不断试错、需要通过大量练习获取经验和值得努力的一个过程。以下将用阿里的百炼大模型平台，介绍提示词工程的一些概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b29f3",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "我们跟大模型交互时使用的是文本数据，但大模型却不是直接于原始的文本数据打交道。在文本进入大模型前会有一个叫做token序列化的过程，它将文本转换成对应的token（也是数字），进行分析和结果输入，同样的大模型在训练时也是使用token作为语料训练的，所以查看输入的文本如何转换成token的将直接影响到大模型的输出结果。\n",
    "对于OPENAI模型，可以使用[OpenAI Tokenizer ](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst)，对于千问这样的模型，可以使用Hugging Face transformers包里的AutoTokenizer实现（对应的模型名称可在[Hugging Facing官网](https://huggingface.co/Qwen)上查到）具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# This script is used to load a tokenizer from the Hugging Face Transformers library.\n",
    "\n",
    "# 加载预训练模型对应的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B\")\n",
    "\n",
    "# 进行分词处理（英文）\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# 输出分词对应的token id\n",
    "print(\"Tokens:\", tokens['input_ids'])\n",
    "\n",
    "# 输出分词结果\n",
    "[print(f\"b[: {tokenizer.decode(token_id)}]\") for token_id in tokens['input_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47173338",
   "metadata": {},
   "source": [
    "可以使用不同的模型，查看一下不同的分词结果。千问也是一个中文模型，那来看一下中文分词结果是如何的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# This script is used to load a tokenizer from the Hugging Face Transformers library.\n",
    "\n",
    "# 加载预训练模型对应的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B\")\n",
    "\n",
    "# 进行分词处理（英文）\n",
    "text = f\"\"\"\n",
    "木星是距离太阳的第五颗行星，也是太阳系中最大的行星。\\\n",
    "它是一颗气态巨行星，质量是太阳的千分之一，但却是太阳系所有其他行星总和的两倍半。\\\n",
    "木星是夜空中肉眼可见的最亮的天体之一，自有记载历史之前就为古代文明所熟知。它以罗马神朱庇特的名字命名。[19]\\\n",
    "从地球上看，木星的亮度足以使其反射的光线投射出可见的阴影，[20]并且平均是夜空中仅次于月球和金星的第三亮自然物体。\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# 输出分词对应的token id\n",
    "print(\"Tokens:\", tokens['input_ids'])\n",
    "\n",
    "# 输出分词结果\n",
    "[print(f\"b[: {tokenizer.decode(token_id)}]\") for token_id in tokens['input_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e527db",
   "metadata": {},
   "source": [
    "跟英文不同，它是根据词来划分的，而英文可以把一个单词拆分到两个token中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0f6ae",
   "metadata": {},
   "source": [
    "## 连接API KEY\n",
    "如果在本地要与厂商提供的大模型进行交互，需要使用厂商提供的API key。一般在练习的时候，需要将API key放入系统环境中，，操作[在此](https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2803795)描述。以下是测试API key有没有加载到系统环境中的测试代码，如果成功，会有一段补全文字数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b624717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a04065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you're referencing the beginning of the U.S. national anthem, **\"The Star-Spangled Banner\"**! Here's the full first verse to continue the lyric:\n",
      "\n",
      "> **\"O say can you see, by the dawn's early light,  \n",
      "> What so proudly we hailed at the twilight's last gleaming?  \n",
      "> Whose broad stripes and bright stars, through the perilous fight,  \n",
      "> O'er the ramparts we watched, were so gallantly streaming?  \n",
      "> And the rocket's red glare, the bombs bursting in air,  \n",
      "> Gave proof through the night that our flag was still there.  \n",
      "> O say, does that star-spangled banner yet wave  \n",
      "> O'er the land of the free and the home of the brave?\"**\n",
      "\n",
      "Let me know if you'd like help with anything related to the anthem, its history, or its lyrics!\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # 环境变量\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt, model=\"qwen-plus\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]       \n",
    "    response = client.chat.completions.create(   \n",
    "        model=model,                                         \n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb34ddf",
   "metadata": {},
   "source": [
    "## 为什么要使用提示词工程\n",
    "虽然用了数据量超大的语料训练这些模型，但是由于语料时序性、大模型自身能力、不同大模型能力差异很大等限制，导致大模型会出现幻觉（提供假信息）、同一输入不同输出等问题。其实重复执行上面代码，对于同一模型（qwen-plus），输入相同得到的输出不同。以下使用不同模型（qwen-plus、deepseek），针对一个不存在的命题（2076年火星战争），设置一个课程计划，其会得到不同的结果，并且也是基于错误得到的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use qwen1.5-0.5b-chat model\n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt, model=\"qwen1.5-0.5b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa118a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use deepseek model\n",
    "response = get_completion(prompt, model=\"deepseek-v3\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06055a8b",
   "metadata": {},
   "source": [
    "对于最近的一些模型（如qwen-plus、deepseek-v3），其对这个不存在问题的幻觉是比较少的，它们给出了一个科幻小说的课程，总体来说是可以接受的。当使用一些较早模型或性能相对较弱的模型时（如qwen1.5-0.5b-chat），就会出现幻觉，当成了一个真实的历史事件进行设计，这显然是不对的，容易让使用的人认为这是真的，不符合Responsible for AI的理念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df434e22",
   "metadata": {},
   "source": [
    "## 基于指导性的提示词工程\n",
    "可以通过设定相关指令，指定大模型的输出形式，如下所示，对一段文字进行总结，并提炼三个关键点，用json的方式进行输出，而对象是以二年级学生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed10383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summary\": \"木星是太阳系中最大的行星，离太阳第五远，看起来非常亮。\",\n",
      "  \"key_points\": [\n",
      "    \"木星是太阳系中最大的行星。\",\n",
      "    \"木星看起来非常亮，古代人们就知道它。\",\n",
      "    \"从地球上看，木星的光可以照出影子。\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "你的任务是用中文一句话总结这段话的主要内容，同时提炼出三个关键点，输出以json格式返回，包含summary和key_points两个字段。记住受众是二年级学生。\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00539e6e",
   "metadata": {},
   "source": [
    "## 多角色系统\n",
    "在使用大模型时，可以设定不同角色的内容，目前主要角色包括system、user和assistant。其中system定义了大模型回复的“基调”，决定了大模型的输出形式，比如是诙谐的还是严肃的等等，user指的就是用户的输入，assistant是一些辅助的信息。而user-assistant可以构建输入输出对，这样可以形成一些上下文的信息，从而让大模型的输出更准确，也可构建one-shot、few-shot系统。\n",
    "以下是多角色系统的示例，可以定义system里不同风格，或者对应的输入输出对："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bd110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2020 World Series was played at **Globe Life Field** in **Arlington, Texas**. Due to the COVID-19 pandemic, the entire series was held at a neutral site without fans in attendance.\n"
     ]
    }
   ],
   "source": [
    "deployment = \"qwen-plus\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# strict assistant\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a strict assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4beg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
