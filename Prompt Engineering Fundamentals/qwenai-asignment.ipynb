{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474fc5a8",
   "metadata": {},
   "source": [
    "# 提示词工程介绍\n",
    "提示词工程是指**设计**和**优化**对大模型的输入内容，其目的让模型能得到一个最优的输出，这是一个不断试错、需要通过大量练习获取经验和值得努力的一个过程。以下将用阿里的百炼大模型平台，介绍提示词工程的一些概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b29f3",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "我们跟大模型交互时使用的是文本数据，但大模型却不是直接于原始的文本数据打交道。在文本进入大模型前会有一个叫做token序列化的过程，它将文本转换成对应的token（也是数字），进行分析和结果输入，同样的大模型在训练时也是使用token作为语料训练的，所以查看输入的文本如何转换成token的将直接影响到大模型的输出结果。\n",
    "对于OPENAI模型，可以使用[OpenAI Tokenizer ](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst)，对于千问这样的模型，可以使用Hugging Face transformers包里的AutoTokenizer实现（对应的模型名称可在[Hugging Facing官网](https://huggingface.co/Qwen)上查到）具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# This script is used to load a tokenizer from the Hugging Face Transformers library.\n",
    "\n",
    "# 加载预训练模型对应的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B\")\n",
    "\n",
    "# 进行分词处理（英文）\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# 输出分词对应的token id\n",
    "print(\"Tokens:\", tokens['input_ids'])\n",
    "\n",
    "# 输出分词结果\n",
    "[print(f\"b[: {tokenizer.decode(token_id)}]\") for token_id in tokens['input_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47173338",
   "metadata": {},
   "source": [
    "可以使用不同的模型，查看一下不同的分词结果。千问也是一个中文模型，那来看一下中文分词结果是如何的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# This script is used to load a tokenizer from the Hugging Face Transformers library.\n",
    "\n",
    "# 加载预训练模型对应的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B\")\n",
    "\n",
    "# 进行分词处理（英文）\n",
    "text = f\"\"\"\n",
    "木星是距离太阳的第五颗行星，也是太阳系中最大的行星。\\\n",
    "它是一颗气态巨行星，质量是太阳的千分之一，但却是太阳系所有其他行星总和的两倍半。\\\n",
    "木星是夜空中肉眼可见的最亮的天体之一，自有记载历史之前就为古代文明所熟知。它以罗马神朱庇特的名字命名。[19]\\\n",
    "从地球上看，木星的亮度足以使其反射的光线投射出可见的阴影，[20]并且平均是夜空中仅次于月球和金星的第三亮自然物体。\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# 输出分词对应的token id\n",
    "print(\"Tokens:\", tokens['input_ids'])\n",
    "\n",
    "# 输出分词结果\n",
    "[print(f\"b[: {tokenizer.decode(token_id)}]\") for token_id in tokens['input_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e527db",
   "metadata": {},
   "source": [
    "跟英文不同，它是根据词来划分的，而英文可以把一个单词拆分到两个token中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0f6ae",
   "metadata": {},
   "source": [
    "## 连接API KEY\n",
    "如果在本地要与厂商提供的大模型进行交互，需要使用厂商提供的API key。一般在练习的时候，需要将API key放入系统环境中，，操作[在此](https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=2803795)描述。以下是测试API key有没有加载到系统环境中的测试代码，如果成功，会有一段补全文字数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b624717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a04065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you're referencing the beginning of the U.S. national anthem, **\"The Star-Spangled Banner\"**! Here's the full first verse to continue the lyric:\n",
      "\n",
      "> **\"O say can you see, by the dawn's early light,  \n",
      "> What so proudly we hailed at the twilight's last gleaming?  \n",
      "> Whose broad stripes and bright stars, through the perilous fight,  \n",
      "> O'er the ramparts we watched, were so gallantly streaming?  \n",
      "> And the rocket's red glare, the bombs bursting in air,  \n",
      "> Gave proof through the night that our flag was still there.  \n",
      "> O say, does that star-spangled banner yet wave  \n",
      "> O'er the land of the free and the home of the brave?\"**\n",
      "\n",
      "Let me know if you'd like help with anything related to the anthem, its history, or its lyrics!\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # 环境变量\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt, model=\"qwen-plus\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]       \n",
    "    response = client.chat.completions.create(   \n",
    "        model=model,                                         \n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb34ddf",
   "metadata": {},
   "source": [
    "## 为什么要使用提示词工程\n",
    "虽然用了数据量超大的语料训练这些模型，但是由于语料时序性、大模型自身能力、不同大模型能力差异很大等限制，导致大模型会出现幻觉（提供假信息）、同一输入不同输出等问题。其实重复执行上面代码，对于同一模型（qwen-plus），输入相同得到的输出不同。以下使用不同模型（qwen-plus、deepseek），针对一个不存在的命题（2076年火星战争），设置一个课程计划，其会得到不同的结果，并且也是基于错误得到的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use qwen1.5-0.5b-chat model\n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt, model=\"qwen1.5-0.5b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa118a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use deepseek model\n",
    "response = get_completion(prompt, model=\"deepseek-v3\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06055a8b",
   "metadata": {},
   "source": [
    "对于最近的一些模型（如qwen-plus、deepseek-v3），其对这个不存在问题的幻觉是比较少的，它们给出了一个科幻小说的课程，总体来说是可以接受的。当使用一些较早模型或性能相对较弱的模型时（如qwen1.5-0.5b-chat），就会出现幻觉，当成了一个真实的历史事件进行设计，这显然是不对的，容易让使用的人认为这是真的，不符合Responsible for AI的理念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df434e22",
   "metadata": {},
   "source": [
    "## 基于指导性的提示词工程\n",
    "可以通过设定相关指令，指定大模型的输出形式，如下所示，对一段文字进行总结，并提炼三个关键点，用json的方式进行输出，而对象是以二年级学生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed10383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summary\": \"木星是太阳系中最大的行星，离太阳第五远，看起来非常亮。\",\n",
      "  \"key_points\": [\n",
      "    \"木星是太阳系中最大的行星。\",\n",
      "    \"木星看起来非常亮，古代人们就知道它。\",\n",
      "    \"从地球上看，木星的光可以照出影子。\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "你的任务是用中文一句话总结这段话的主要内容，同时提炼出三个关键点，输出以json格式返回，包含summary和key_points两个字段。记住受众是二年级学生。\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00539e6e",
   "metadata": {},
   "source": [
    "## 多角色系统\n",
    "在使用大模型时，可以设定不同角色的内容，目前主要角色包括system、user和assistant。其中system定义了大模型回复的“基调”，决定了大模型的输出形式，比如是诙谐的还是严肃的等等，user指的就是用户的输入，assistant是一些辅助的信息。而user-assistant可以构建输入输出对，这样可以形成一些上下文的信息，从而让大模型的输出更准确，也可构建one-shot、few-shot系统。\n",
    "以下是多角色系统的示例，可以定义system里不同风格，或者对应的输入输出对："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "163bd110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, were you not watching? It was played at a secret, undisclosed location... known only as \"the ESPN Wide World of Sports Complex in Orlando, Florida.\" Yep, MLB turned Disney World into a baseball bubble. Because nothing says \"World Series magic\" like a championship game next to a Goofy-themed parking lot. 🏀⚾ Wait, was that the NBA or the MLB? ...Actually, both in 2020. Priorities, people.\n",
      "The 2020 World Series was played at **Dodger Stadium in Los Angeles, California**, and **Globe Life Field in Arlington, Texas**. \n",
      "\n",
      "Due to the COVID-19 pandemic, the format was altered, and Games 3 through 6 were held at a neutral site: **Globe Life Field** in Texas, which was the home stadium of the Texas Rangers and hosted the American League teams (the Tampa Bay Rays) as the designated \"home\" team for those games.\n"
     ]
    }
   ],
   "source": [
    "deployment = \"qwen-plus\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# strict assistant\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a strict assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbfc51",
   "metadata": {},
   "source": [
    "## 对指令行动进行补充的内容\n",
    "### 利用一些例子\n",
    "可以使用一些例子辅助指令，这样可以让结果更符合你想要的。以下例子就是提供一些回答各年NBA冠军的的例子，使得回答更规范："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f678a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020年NBA总冠军是**洛杉矶湖人队**（Los Angeles Lakers）。\n",
      "\n",
      "那一年，湖人队在**NBA总决赛**中以总比分**4比2**击败了**迈阿密热火队**（Miami Heat），成功夺得了队史第17座总冠军奖杯，追平了波士顿凯尔特人队的历史纪录。\n",
      "\n",
      "### 核心球员表现：\n",
      "- **勒布朗·詹姆斯**（LeBron James）：场均29.8分、11.8篮板、8.5助攻，荣获**总决赛MVP**（FMVP）。\n",
      "- **安东尼·戴维斯**（Anthony Davis）：在系列赛中发挥出色，是湖人内线的支柱。\n",
      "- 其他关键球员包括**丹尼·格林**、**肯塔维厄斯·卡德威尔-波普**、**拉简·隆多**、**德怀特·霍华德**、**贾维尔·麦基**等。\n",
      "\n",
      "### 2020年NBA赛季背景：\n",
      "由于新冠疫情，2019-2020赛季中途暂停，并于2020年7月在**奥兰多迪士尼园区**（俗称“泡泡”）复赛。整个季后赛和总决赛都在这个封闭环境中进行。\n",
      "\n",
      "湖人队夺冠也被视为对已故球星**科比·布莱恩特**（Kobe Bryant）的致敬，他在2020年1月不幸去世。\n",
      "\n",
      "如你有兴趣，我也可以提供当年总决赛每场比赛的详细比分和亮点。\n",
      "2020年NBA总冠军是**洛杉矶湖人队**。他们在总决赛中以**4-2**的比分击败了**迈阿密热火队**，赢得了队史第17座总冠军奖杯，与波士顿凯尔特人队并列成为当时NBA历史上夺冠次数最多的球队。  \n",
      "\n",
      "**关键细节：**  \n",
      "1. **FMVP**：勒布朗·詹姆斯（生涯第4次当选）。  \n",
      "2. **特殊背景**：因新冠疫情，赛季一度中断，总决赛在奥兰多的封闭园区（\"泡泡\"）内进行。  \n",
      "3. **意义**：湖人队自2010年后时隔10年再次夺冠，也是科比·布莱恩特意外离世后球队的首冠，全队将其献给科比。  \n",
      "\n",
      "如果需要更深入的比赛分析或历史背景，可以进一步补充！\n"
     ]
    }
   ],
   "source": [
    "# 没有例子进行提示的回答，其答案不同的模型可能会有不同的结果\n",
    "zero_shot_prompt = f\"\"\"2020年NBA总冠军是哪支球队？\"\"\"\n",
    "\n",
    "zero_shot_prompt_response_qw = get_completion(zero_shot_prompt, model=\"qwen-plus\")\n",
    "print(zero_shot_prompt_response_qw)\n",
    "\n",
    "zero_shot_prompt_response_deepseek = get_completion(zero_shot_prompt, model=\"deepseek-v3\")\n",
    "print(zero_shot_prompt_response_deepseek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd89b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020年NBA总冠军是洛杉矶湖人队。\n",
      "2020年NBA总冠军是洛杉矶湖人队。\n"
     ]
    }
   ],
   "source": [
    "# 利用一些例子进行提示，这样使用不同的模型回答就是一致的\n",
    "few_shot_prompt = f\"\"\"请根据以下示例，回答某一年NBA总冠军是哪支球队？\\\n",
    "示例1：2016年NBA总冠军是克利夫兰骑士队。\\\n",
    "示例2：2017年NBA总冠军是金州勇士队。\\\n",
    "示例3：2018年NBA总冠军是金州勇士队。\\\n",
    "现在请回答：2020年NBA总冠军是哪支球队？\"\"\"\n",
    "\n",
    "few_shot_prompt_response_qw = get_completion(few_shot_prompt, model=\"qwen-plus\")\n",
    "print(few_shot_prompt_response_qw)\n",
    "\n",
    "few_shot_prompt_response_deepseek = get_completion(few_shot_prompt, model=\"deepseek-v3\")\n",
    "print(few_shot_prompt_response_deepseek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de125b",
   "metadata": {},
   "source": [
    "### 给出一些提示\n",
    "可以给大模型一些提示而不是使用例子，也能使大模型输出更为准确的内容。还是上面回答NBA总冠军的例子，这次针对不同的模型，使用提示的方式，看看效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df73a230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020年NBA总冠军：**洛杉矶湖人队（Los Angeles Lakers）**  \n",
      "MVP球员：**勒布朗·詹姆斯（LeBron James）**\n",
      "2020年NBA总冠军是**洛杉矶湖人队**，总决赛MVP是**勒布朗·詹姆斯**。\n"
     ]
    }
   ],
   "source": [
    "cue_prompt = f\"\"\"2020年NBA总冠军是哪支球队？只需列出冠军球队和MVP球员的名字\"\"\"\n",
    "\n",
    "cue_prompt_response_qw = get_completion(cue_prompt, model=\"qwen-plus\")\n",
    "print(cue_prompt_response_qw)\n",
    "\n",
    "cue_prompt_response_deepseek = get_completion(cue_prompt, model=\"deepseek-v3\")\n",
    "print(cue_prompt_response_deepseek)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4beg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
